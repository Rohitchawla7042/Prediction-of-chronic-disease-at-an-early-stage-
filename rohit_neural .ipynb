{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import io "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data1.csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8670</td>\n",
       "      <td>B</td>\n",
       "      <td>13.050</td>\n",
       "      <td>19.31</td>\n",
       "      <td>82.61</td>\n",
       "      <td>527.2</td>\n",
       "      <td>0.08060</td>\n",
       "      <td>0.03789</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>...</td>\n",
       "      <td>14.23</td>\n",
       "      <td>22.25</td>\n",
       "      <td>90.24</td>\n",
       "      <td>624.1</td>\n",
       "      <td>0.10210</td>\n",
       "      <td>0.06191</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>0.011110</td>\n",
       "      <td>0.2439</td>\n",
       "      <td>0.06289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8913</td>\n",
       "      <td>B</td>\n",
       "      <td>12.540</td>\n",
       "      <td>18.07</td>\n",
       "      <td>79.42</td>\n",
       "      <td>491.9</td>\n",
       "      <td>0.07436</td>\n",
       "      <td>0.02650</td>\n",
       "      <td>0.001194</td>\n",
       "      <td>0.005449</td>\n",
       "      <td>...</td>\n",
       "      <td>13.72</td>\n",
       "      <td>20.98</td>\n",
       "      <td>86.82</td>\n",
       "      <td>585.7</td>\n",
       "      <td>0.09293</td>\n",
       "      <td>0.04327</td>\n",
       "      <td>0.003581</td>\n",
       "      <td>0.016350</td>\n",
       "      <td>0.2233</td>\n",
       "      <td>0.05521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8915</td>\n",
       "      <td>B</td>\n",
       "      <td>11.330</td>\n",
       "      <td>14.16</td>\n",
       "      <td>71.79</td>\n",
       "      <td>396.6</td>\n",
       "      <td>0.09379</td>\n",
       "      <td>0.03872</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>...</td>\n",
       "      <td>12.20</td>\n",
       "      <td>18.99</td>\n",
       "      <td>77.37</td>\n",
       "      <td>458.0</td>\n",
       "      <td>0.12590</td>\n",
       "      <td>0.07348</td>\n",
       "      <td>0.004955</td>\n",
       "      <td>0.011110</td>\n",
       "      <td>0.2758</td>\n",
       "      <td>0.06386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9047</td>\n",
       "      <td>B</td>\n",
       "      <td>11.250</td>\n",
       "      <td>14.78</td>\n",
       "      <td>71.38</td>\n",
       "      <td>390.0</td>\n",
       "      <td>0.08306</td>\n",
       "      <td>0.04458</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.002941</td>\n",
       "      <td>...</td>\n",
       "      <td>12.76</td>\n",
       "      <td>22.06</td>\n",
       "      <td>82.08</td>\n",
       "      <td>492.7</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.09794</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>0.016670</td>\n",
       "      <td>0.2815</td>\n",
       "      <td>0.07418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85715</td>\n",
       "      <td>B</td>\n",
       "      <td>12.580</td>\n",
       "      <td>18.40</td>\n",
       "      <td>79.83</td>\n",
       "      <td>489.0</td>\n",
       "      <td>0.08393</td>\n",
       "      <td>0.04216</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.002924</td>\n",
       "      <td>...</td>\n",
       "      <td>13.50</td>\n",
       "      <td>23.08</td>\n",
       "      <td>85.56</td>\n",
       "      <td>564.1</td>\n",
       "      <td>0.10380</td>\n",
       "      <td>0.06624</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.2505</td>\n",
       "      <td>0.06431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>911157302</td>\n",
       "      <td>M</td>\n",
       "      <td>20.600</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.351400</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>...</td>\n",
       "      <td>25.74</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.938700</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>911296201</td>\n",
       "      <td>M</td>\n",
       "      <td>22.270</td>\n",
       "      <td>19.67</td>\n",
       "      <td>152.80</td>\n",
       "      <td>1509.0</td>\n",
       "      <td>0.13260</td>\n",
       "      <td>0.27680</td>\n",
       "      <td>0.426400</td>\n",
       "      <td>0.182300</td>\n",
       "      <td>...</td>\n",
       "      <td>28.40</td>\n",
       "      <td>28.01</td>\n",
       "      <td>206.80</td>\n",
       "      <td>2360.0</td>\n",
       "      <td>0.17010</td>\n",
       "      <td>0.69970</td>\n",
       "      <td>0.960800</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.4055</td>\n",
       "      <td>0.09789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>911296202</td>\n",
       "      <td>M</td>\n",
       "      <td>12.460</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.227300</td>\n",
       "      <td>0.085430</td>\n",
       "      <td>...</td>\n",
       "      <td>15.09</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.18530</td>\n",
       "      <td>1.05800</td>\n",
       "      <td>1.105000</td>\n",
       "      <td>0.221000</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>911320501</td>\n",
       "      <td>M</td>\n",
       "      <td>15.220</td>\n",
       "      <td>30.62</td>\n",
       "      <td>103.40</td>\n",
       "      <td>716.9</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.20870</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.094290</td>\n",
       "      <td>...</td>\n",
       "      <td>17.52</td>\n",
       "      <td>42.79</td>\n",
       "      <td>128.70</td>\n",
       "      <td>915.0</td>\n",
       "      <td>0.14170</td>\n",
       "      <td>0.79170</td>\n",
       "      <td>1.170000</td>\n",
       "      <td>0.235600</td>\n",
       "      <td>0.4089</td>\n",
       "      <td>0.14090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>911320502</td>\n",
       "      <td>B</td>\n",
       "      <td>9.029</td>\n",
       "      <td>17.33</td>\n",
       "      <td>58.79</td>\n",
       "      <td>250.5</td>\n",
       "      <td>0.10660</td>\n",
       "      <td>0.14130</td>\n",
       "      <td>0.313000</td>\n",
       "      <td>0.043750</td>\n",
       "      <td>...</td>\n",
       "      <td>10.31</td>\n",
       "      <td>22.65</td>\n",
       "      <td>65.50</td>\n",
       "      <td>324.7</td>\n",
       "      <td>0.14820</td>\n",
       "      <td>0.43650</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.4228</td>\n",
       "      <td>0.11750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>556 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id diagnosis  radius_mean  texture_mean  perimeter_mean  \\\n",
       "0         8670         B       13.050         19.31           82.61   \n",
       "1         8913         B       12.540         18.07           79.42   \n",
       "2         8915         B       11.330         14.16           71.79   \n",
       "3         9047         B       11.250         14.78           71.38   \n",
       "4        85715         B       12.580         18.40           79.83   \n",
       "..         ...       ...          ...           ...             ...   \n",
       "551  911157302         M       20.600         29.33          140.10   \n",
       "552  911296201         M       22.270         19.67          152.80   \n",
       "553  911296202         M       12.460         24.04           83.97   \n",
       "554  911320501         M       15.220         30.62          103.40   \n",
       "555  911320502         B        9.029         17.33           58.79   \n",
       "\n",
       "     area_mean  smoothness_mean  compactness_mean  concavity_mean  \\\n",
       "0        527.2          0.08060           0.03789        0.000692   \n",
       "1        491.9          0.07436           0.02650        0.001194   \n",
       "2        396.6          0.09379           0.03872        0.001487   \n",
       "3        390.0          0.08306           0.04458        0.000974   \n",
       "4        489.0          0.08393           0.04216        0.001860   \n",
       "..         ...              ...               ...             ...   \n",
       "551     1265.0          0.11780           0.27700        0.351400   \n",
       "552     1509.0          0.13260           0.27680        0.426400   \n",
       "553      475.9          0.11860           0.23960        0.227300   \n",
       "554      716.9          0.10480           0.20870        0.255000   \n",
       "555      250.5          0.10660           0.14130        0.313000   \n",
       "\n",
       "     concave points_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0               0.004167  ...         14.23          22.25            90.24   \n",
       "1               0.005449  ...         13.72          20.98            86.82   \n",
       "2               0.003333  ...         12.20          18.99            77.37   \n",
       "3               0.002941  ...         12.76          22.06            82.08   \n",
       "4               0.002924  ...         13.50          23.08            85.56   \n",
       "..                   ...  ...           ...            ...              ...   \n",
       "551             0.152000  ...         25.74          39.42           184.60   \n",
       "552             0.182300  ...         28.40          28.01           206.80   \n",
       "553             0.085430  ...         15.09          40.68            97.65   \n",
       "554             0.094290  ...         17.52          42.79           128.70   \n",
       "555             0.043750  ...         10.31          22.65            65.50   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0         624.1           0.10210            0.06191         0.001845   \n",
       "1         585.7           0.09293            0.04327         0.003581   \n",
       "2         458.0           0.12590            0.07348         0.004955   \n",
       "3         492.7           0.11660            0.09794         0.005518   \n",
       "4         564.1           0.10380            0.06624         0.005579   \n",
       "..          ...               ...                ...              ...   \n",
       "551      1821.0           0.16500            0.86810         0.938700   \n",
       "552      2360.0           0.17010            0.69970         0.960800   \n",
       "553       711.4           0.18530            1.05800         1.105000   \n",
       "554       915.0           0.14170            0.79170         1.170000   \n",
       "555       324.7           0.14820            0.43650         1.252000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.011110          0.2439                  0.06289  \n",
       "1                0.016350          0.2233                  0.05521  \n",
       "2                0.011110          0.2758                  0.06386  \n",
       "3                0.016670          0.2815                  0.07418  \n",
       "4                0.008772          0.2505                  0.06431  \n",
       "..                    ...             ...                      ...  \n",
       "551              0.265000          0.4087                  0.12400  \n",
       "552              0.291000          0.4055                  0.09789  \n",
       "553              0.221000          0.4366                  0.20750  \n",
       "554              0.235600          0.4089                  0.14090  \n",
       "555              0.175000          0.4228                  0.11750  \n",
       "\n",
       "[556 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                         0\n",
       "diagnosis                  0\n",
       "radius_mean                0\n",
       "texture_mean               0\n",
       "perimeter_mean             0\n",
       "area_mean                  0\n",
       "smoothness_mean            0\n",
       "compactness_mean           0\n",
       "concavity_mean             0\n",
       "concave points_mean        0\n",
       "symmetry_mean              0\n",
       "fractal_dimension_mean     0\n",
       "radius_se                  0\n",
       "texture_se                 0\n",
       "perimeter_se               0\n",
       "area_se                    0\n",
       "smoothness_se              0\n",
       "compactness_se             0\n",
       "concavity_se               0\n",
       "concave points_se          0\n",
       "symmetry_se                0\n",
       "fractal_dimension_se       0\n",
       "radius_worst               0\n",
       "texture_worst              0\n",
       "perimeter_worst            0\n",
       "area_worst                 0\n",
       "smoothness_worst           0\n",
       "compactness_worst          1\n",
       "concavity_worst            0\n",
       "concave points_worst       0\n",
       "symmetry_worst             0\n",
       "fractal_dimension_worst    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                         0\n",
       "diagnosis                  0\n",
       "radius_mean                0\n",
       "texture_mean               0\n",
       "perimeter_mean             0\n",
       "area_mean                  0\n",
       "smoothness_mean            0\n",
       "compactness_mean           0\n",
       "concavity_mean             0\n",
       "concave points_mean        0\n",
       "symmetry_mean              0\n",
       "fractal_dimension_mean     0\n",
       "radius_se                  0\n",
       "texture_se                 0\n",
       "perimeter_se               0\n",
       "area_se                    0\n",
       "smoothness_se              0\n",
       "compactness_se             0\n",
       "concavity_se               0\n",
       "concave points_se          0\n",
       "symmetry_se                0\n",
       "fractal_dimension_se       0\n",
       "radius_worst               0\n",
       "texture_worst              0\n",
       "perimeter_worst            0\n",
       "area_worst                 0\n",
       "smoothness_worst           0\n",
       "compactness_worst          0\n",
       "concavity_worst            0\n",
       "concave points_worst       0\n",
       "symmetry_worst             0\n",
       "fractal_dimension_worst    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B    343\n",
       "M    212\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2a143992688>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data['diagnosis'], label ='count' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the categorical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sidharth\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "551    1\n",
       "552    1\n",
       "553    1\n",
       "554    1\n",
       "555    0\n",
       "Name: diagnosis, Length: 555, dtype: int32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_Y = LabelEncoder()\n",
    "data.iloc[:,1] =labelencoder_Y.fit_transform(data.iloc[:,1].values)  # 0 = B(Benign)  1 = M(Maliganant)\n",
    "\n",
    "data.iloc[:,1]\n",
    "# labelencoder_X_1 = LabelEncoder()\n",
    "# y = labelencoder_X_1.fit_transform(y)\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into independent(X) and dependent (Y)\n",
    "X = data.iloc[:,2:31].values       # it will tell us that the features can detect that whether the patient has cancer or not \n",
    "y = data.iloc[:,1].values          # it will tell us that whether the pateint is suffering from cancer or not \n",
    "\n",
    "# X = data.iloc[:, 2:].values  # independent variable startting from column 2 to last column \n",
    "# y = data.iloc[:, 1].values  # dependent variable that is diagnosis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)   #0.1 means that the 10% testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling as because we make our data statiscally correct as we can see that the radius mean having the values in tens but the area_mean having the value in hundreds so we scale the data from +1 to the -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data (Feature Scaling )\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)\n",
    "\n",
    "# # Scale the data (Feature Scaling )\n",
    "# from sklearn.preprocessing import StandardScaler \n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.fit_transform(X_test)\n",
    "\n",
    "# # from sklearn.preprocessing import StandardScaler \n",
    "# # sc = StandardScaler()\n",
    "# # X_train = sc.fit_transform(X_train)\n",
    "# # X_test = sc.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.29126786,  2.53503122,  0.20293759, ..., -0.58501599,\n",
       "        -0.78335132,  0.59139134],\n",
       "       [ 0.47005997,  0.01744974,  0.63942954, ...,  1.96254786,\n",
       "         1.55046411,  2.31853971],\n",
       "       [-0.75264738,  1.13716728, -0.72928151, ...,  0.41152401,\n",
       "        -0.29715951, -1.32178389],\n",
       "       ...,\n",
       "       [-1.31295879, -0.76944306, -1.3304557 , ..., -0.98253163,\n",
       "        -0.6811531 , -0.2209828 ],\n",
       "       [-1.11599909, -0.70525543, -1.06536771, ..., -0.41709206,\n",
       "        -0.28278542, -0.47527639],\n",
       "       [-1.02371929, -0.19650903, -1.05276448, ..., -1.13442863,\n",
       "        -1.47093836, -0.31996957]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03564868, -1.3787365 , -0.07520266, ..., -0.17652474,\n",
       "        -0.49988904, -0.51074841],\n",
       "       [ 0.66998987, -0.13773132,  0.7046205 , ...,  1.10312117,\n",
       "         1.94155991,  0.14882271],\n",
       "       [-1.85945295, -0.65061907, -1.8127678 , ..., -1.09041279,\n",
       "        -1.49516576,  0.14336042],\n",
       "       ...,\n",
       "       [ 0.50143653,  0.09810546,  0.5373115 , ...,  1.53068298,\n",
       "         1.17804349,  0.07371627],\n",
       "       [-1.321225  , -1.04673327, -1.32104257, ..., -1.40389058,\n",
       "        -1.72392009, -0.09151791],\n",
       "       [ 1.42419464,  0.35912869,  1.3820179 , ...,  0.842734  ,\n",
       "         1.7717226 ,  0.83707084]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_dim = 30 this means this 30 is the independent layer \n",
    "output_dim = 16 this means that we have 16 hidden layers how this 16 comes that we have 31 independent layers so we divide 31 by 2 so we have 15.25 so we round off that 15.25 to 16\n",
    "as when we add the output layer so we make the activation sigmoid  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sidharth\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=30, units=16, kernel_initializer=\"uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# adding the input and first hidden layer \n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(output_dim=16, init= 'uniform', activation='relu',input_dim=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sidharth\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=16, kernel_initializer=\"uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# adding the second hidden layer \n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(output_dim=16, init= 'uniform', activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sidharth\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# adding the output  layer \n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(output_dim=1, init= 'uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='Adam',loss= 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sidharth\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "444/444 [==============================] - 0s 516us/step - loss: 0.6218 - accuracy: 0.8131\n",
      "Epoch 2/150\n",
      "444/444 [==============================] - 0s 45us/step - loss: 0.5945 - accuracy: 0.8559\n",
      "Epoch 3/150\n",
      "444/444 [==============================] - 0s 45us/step - loss: 0.5686 - accuracy: 0.8716\n",
      "Epoch 4/150\n",
      "444/444 [==============================] - 0s 27us/step - loss: 0.5441 - accuracy: 0.8874\n",
      "Epoch 5/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.5219 - accuracy: 0.8964\n",
      "Epoch 6/150\n",
      "444/444 [==============================] - 0s 45us/step - loss: 0.5009 - accuracy: 0.9032\n",
      "Epoch 7/150\n",
      "444/444 [==============================] - 0s 27us/step - loss: 0.4818 - accuracy: 0.9099\n",
      "Epoch 8/150\n",
      "444/444 [==============================] - ETA: 0s - loss: 0.4723 - accuracy: 0.93 - 0s 45us/step - loss: 0.4638 - accuracy: 0.9167\n",
      "Epoch 9/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.4475 - accuracy: 0.9212\n",
      "Epoch 10/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.4320 - accuracy: 0.9212\n",
      "Epoch 11/150\n",
      "444/444 [==============================] - 0s 45us/step - loss: 0.4180 - accuracy: 0.9234\n",
      "Epoch 12/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.4049 - accuracy: 0.9257\n",
      "Epoch 13/150\n",
      "444/444 [==============================] - 0s 27us/step - loss: 0.3926 - accuracy: 0.9279\n",
      "Epoch 14/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.3811 - accuracy: 0.9324\n",
      "Epoch 15/150\n",
      "444/444 [==============================] - 0s 27us/step - loss: 0.3706 - accuracy: 0.9324\n",
      "Epoch 16/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.3604 - accuracy: 0.9324\n",
      "Epoch 17/150\n",
      "444/444 [==============================] - 0s 37us/step - loss: 0.3510 - accuracy: 0.9369\n",
      "Epoch 18/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.3421 - accuracy: 0.9392\n",
      "Epoch 19/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.3337 - accuracy: 0.9392\n",
      "Epoch 20/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.3259 - accuracy: 0.9414\n",
      "Epoch 21/150\n",
      "444/444 [==============================] - 0s 45us/step - loss: 0.3184 - accuracy: 0.9414\n",
      "Epoch 22/150\n",
      "444/444 [==============================] - 0s 52us/step - loss: 0.3113 - accuracy: 0.9414\n",
      "Epoch 23/150\n",
      "444/444 [==============================] - 0s 27us/step - loss: 0.3045 - accuracy: 0.9459\n",
      "Epoch 24/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.2981 - accuracy: 0.9482\n",
      "Epoch 25/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.2920 - accuracy: 0.9527\n",
      "Epoch 26/150\n",
      "444/444 [==============================] - 0s 29us/step - loss: 0.2862 - accuracy: 0.9527\n",
      "Epoch 27/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.2806 - accuracy: 0.9527\n",
      "Epoch 28/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.2752 - accuracy: 0.9527\n",
      "Epoch 29/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.2702 - accuracy: 0.9527\n",
      "Epoch 30/150\n",
      "444/444 [==============================] - 0s 45us/step - loss: 0.2653 - accuracy: 0.9527\n",
      "Epoch 31/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.2607 - accuracy: 0.9550\n",
      "Epoch 32/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.2563 - accuracy: 0.9572\n",
      "Epoch 33/150\n",
      "444/444 [==============================] - 0s 45us/step - loss: 0.2520 - accuracy: 0.9572\n",
      "Epoch 34/150\n",
      "444/444 [==============================] - 0s 35us/step - loss: 0.2479 - accuracy: 0.9572\n",
      "Epoch 35/150\n",
      "444/444 [==============================] - 0s 45us/step - loss: 0.2439 - accuracy: 0.9572\n",
      "Epoch 36/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.2402 - accuracy: 0.9572\n",
      "Epoch 37/150\n",
      "444/444 [==============================] - 0s 45us/step - loss: 0.2366 - accuracy: 0.9572\n",
      "Epoch 38/150\n",
      "444/444 [==============================] - 0s 46us/step - loss: 0.2330 - accuracy: 0.9595\n",
      "Epoch 39/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.2296 - accuracy: 0.9595\n",
      "Epoch 40/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.2264 - accuracy: 0.9595\n",
      "Epoch 41/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.2232 - accuracy: 0.9595\n",
      "Epoch 42/150\n",
      "444/444 [==============================] - ETA: 0s - loss: 0.2173 - accuracy: 0.97 - 0s 45us/step - loss: 0.2202 - accuracy: 0.9617\n",
      "Epoch 43/150\n",
      "444/444 [==============================] - 0s 45us/step - loss: 0.2173 - accuracy: 0.9640\n",
      "Epoch 44/150\n",
      "444/444 [==============================] - 0s 45us/step - loss: 0.2144 - accuracy: 0.9640\n",
      "Epoch 45/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.2116 - accuracy: 0.9640\n",
      "Epoch 46/150\n",
      "444/444 [==============================] - 0s 27us/step - loss: 0.2090 - accuracy: 0.9640\n",
      "Epoch 47/150\n",
      "444/444 [==============================] - 0s 27us/step - loss: 0.2064 - accuracy: 0.9640\n",
      "Epoch 48/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.2039 - accuracy: 0.9640\n",
      "Epoch 49/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.2014 - accuracy: 0.9640\n",
      "Epoch 50/150\n",
      "444/444 [==============================] - 0s 27us/step - loss: 0.1991 - accuracy: 0.9662\n",
      "Epoch 51/150\n",
      "444/444 [==============================] - 0s 35us/step - loss: 0.1968 - accuracy: 0.9662\n",
      "Epoch 52/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1946 - accuracy: 0.9662\n",
      "Epoch 53/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1925 - accuracy: 0.9662\n",
      "Epoch 54/150\n",
      "444/444 [==============================] - 0s 27us/step - loss: 0.1904 - accuracy: 0.9662\n",
      "Epoch 55/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1884 - accuracy: 0.9685\n",
      "Epoch 56/150\n",
      "444/444 [==============================] - 0s 46us/step - loss: 0.1865 - accuracy: 0.9707\n",
      "Epoch 57/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1846 - accuracy: 0.9730\n",
      "Epoch 58/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1827 - accuracy: 0.9752\n",
      "Epoch 59/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1810 - accuracy: 0.9752\n",
      "Epoch 60/150\n",
      "444/444 [==============================] - 0s 28us/step - loss: 0.1792 - accuracy: 0.9752\n",
      "Epoch 61/150\n",
      "444/444 [==============================] - 0s 27us/step - loss: 0.1774 - accuracy: 0.9752\n",
      "Epoch 62/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1758 - accuracy: 0.9752\n",
      "Epoch 63/150\n",
      "444/444 [==============================] - ETA: 0s - loss: 0.1660 - accuracy: 0.99 - 0s 45us/step - loss: 0.1742 - accuracy: 0.9752\n",
      "Epoch 64/150\n",
      "444/444 [==============================] - 0s 45us/step - loss: 0.1726 - accuracy: 0.9752\n",
      "Epoch 65/150\n",
      "444/444 [==============================] - 0s 43us/step - loss: 0.1710 - accuracy: 0.9752\n",
      "Epoch 66/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1695 - accuracy: 0.9752\n",
      "Epoch 67/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1680 - accuracy: 0.9752\n",
      "Epoch 68/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1666 - accuracy: 0.9752\n",
      "Epoch 69/150\n",
      "444/444 [==============================] - 0s 38us/step - loss: 0.1652 - accuracy: 0.9752\n",
      "Epoch 70/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1638 - accuracy: 0.9752\n",
      "Epoch 71/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1625 - accuracy: 0.9752\n",
      "Epoch 72/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1611 - accuracy: 0.9752\n",
      "Epoch 73/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1599 - accuracy: 0.9752\n",
      "Epoch 74/150\n",
      "444/444 [==============================] - 0s 25us/step - loss: 0.1586 - accuracy: 0.9752\n",
      "Epoch 75/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1574 - accuracy: 0.9752\n",
      "Epoch 76/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1562 - accuracy: 0.9752\n",
      "Epoch 77/150\n",
      "444/444 [==============================] - 0s 27us/step - loss: 0.1550 - accuracy: 0.9752\n",
      "Epoch 78/150\n",
      "444/444 [==============================] - 0s 20us/step - loss: 0.1539 - accuracy: 0.9752\n",
      "Epoch 79/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444/444 [==============================] - 0s 36us/step - loss: 0.1528 - accuracy: 0.9752\n",
      "Epoch 80/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1517 - accuracy: 0.9752\n",
      "Epoch 81/150\n",
      "444/444 [==============================] - 0s 27us/step - loss: 0.1506 - accuracy: 0.9775\n",
      "Epoch 82/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1495 - accuracy: 0.9775\n",
      "Epoch 83/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1485 - accuracy: 0.9775\n",
      "Epoch 84/150\n",
      "444/444 [==============================] - 0s 45us/step - loss: 0.1475 - accuracy: 0.9775\n",
      "Epoch 85/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1465 - accuracy: 0.9775\n",
      "Epoch 86/150\n",
      "444/444 [==============================] - 0s 45us/step - loss: 0.1455 - accuracy: 0.9775\n",
      "Epoch 87/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1446 - accuracy: 0.9775\n",
      "Epoch 88/150\n",
      "444/444 [==============================] - 0s 54us/step - loss: 0.1437 - accuracy: 0.9775\n",
      "Epoch 89/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1428 - accuracy: 0.9775\n",
      "Epoch 90/150\n",
      "444/444 [==============================] - 0s 27us/step - loss: 0.1419 - accuracy: 0.9775\n",
      "Epoch 91/150\n",
      "444/444 [==============================] - 0s 45us/step - loss: 0.1410 - accuracy: 0.9775\n",
      "Epoch 92/150\n",
      "444/444 [==============================] - 0s 39us/step - loss: 0.1401 - accuracy: 0.9775\n",
      "Epoch 93/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1393 - accuracy: 0.9775\n",
      "Epoch 94/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1385 - accuracy: 0.9775\n",
      "Epoch 95/150\n",
      "444/444 [==============================] - 0s 27us/step - loss: 0.1377 - accuracy: 0.9797\n",
      "Epoch 96/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1369 - accuracy: 0.9797\n",
      "Epoch 97/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1361 - accuracy: 0.9797\n",
      "Epoch 98/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1353 - accuracy: 0.9797\n",
      "Epoch 99/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1346 - accuracy: 0.9797\n",
      "Epoch 100/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1338 - accuracy: 0.9797\n",
      "Epoch 101/150\n",
      "444/444 [==============================] - 0s 27us/step - loss: 0.1331 - accuracy: 0.9797\n",
      "Epoch 102/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1323 - accuracy: 0.9797\n",
      "Epoch 103/150\n",
      "444/444 [==============================] - 0s 45us/step - loss: 0.1316 - accuracy: 0.9797\n",
      "Epoch 104/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1309 - accuracy: 0.9797\n",
      "Epoch 105/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1302 - accuracy: 0.9797\n",
      "Epoch 106/150\n",
      "444/444 [==============================] - 0s 34us/step - loss: 0.1295 - accuracy: 0.9797\n",
      "Epoch 107/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1289 - accuracy: 0.9797\n",
      "Epoch 108/150\n",
      "444/444 [==============================] - 0s 27us/step - loss: 0.1282 - accuracy: 0.9797\n",
      "Epoch 109/150\n",
      "444/444 [==============================] - 0s 45us/step - loss: 0.1276 - accuracy: 0.9797\n",
      "Epoch 110/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1269 - accuracy: 0.9797\n",
      "Epoch 111/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1263 - accuracy: 0.9797\n",
      "Epoch 112/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1257 - accuracy: 0.9797\n",
      "Epoch 113/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1251 - accuracy: 0.9797\n",
      "Epoch 114/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1245 - accuracy: 0.9797\n",
      "Epoch 115/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1239 - accuracy: 0.9797\n",
      "Epoch 116/150\n",
      "444/444 [==============================] - 0s 27us/step - loss: 0.1233 - accuracy: 0.9797\n",
      "Epoch 117/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1228 - accuracy: 0.9797\n",
      "Epoch 118/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1222 - accuracy: 0.9797\n",
      "Epoch 119/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1217 - accuracy: 0.9797\n",
      "Epoch 120/150\n",
      "444/444 [==============================] - 0s 18us/step - loss: 0.1211 - accuracy: 0.9797\n",
      "Epoch 121/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1206 - accuracy: 0.9797\n",
      "Epoch 122/150\n",
      "444/444 [==============================] - 0s 27us/step - loss: 0.1201 - accuracy: 0.9797\n",
      "Epoch 123/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1196 - accuracy: 0.9797\n",
      "Epoch 124/150\n",
      "444/444 [==============================] - 0s 27us/step - loss: 0.1191 - accuracy: 0.9797\n",
      "Epoch 125/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1186 - accuracy: 0.9797\n",
      "Epoch 126/150\n",
      "444/444 [==============================] - 0s 57us/step - loss: 0.1181 - accuracy: 0.9797\n",
      "Epoch 127/150\n",
      "444/444 [==============================] - 0s 39us/step - loss: 0.1176 - accuracy: 0.9797\n",
      "Epoch 128/150\n",
      "444/444 [==============================] - 0s 32us/step - loss: 0.1171 - accuracy: 0.9797\n",
      "Epoch 129/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1166 - accuracy: 0.9797\n",
      "Epoch 130/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1162 - accuracy: 0.9797\n",
      "Epoch 131/150\n",
      "444/444 [==============================] - 0s 27us/step - loss: 0.1157 - accuracy: 0.9797\n",
      "Epoch 132/150\n",
      "444/444 [==============================] - 0s 54us/step - loss: 0.1152 - accuracy: 0.9797\n",
      "Epoch 133/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1148 - accuracy: 0.9797\n",
      "Epoch 134/150\n",
      "444/444 [==============================] - 0s 27us/step - loss: 0.1143 - accuracy: 0.9820\n",
      "Epoch 135/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1139 - accuracy: 0.9820\n",
      "Epoch 136/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1135 - accuracy: 0.9820\n",
      "Epoch 137/150\n",
      "444/444 [==============================] - 0s 45us/step - loss: 0.1130 - accuracy: 0.9820\n",
      "Epoch 138/150\n",
      "444/444 [==============================] - 0s 45us/step - loss: 0.1126 - accuracy: 0.9820\n",
      "Epoch 139/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1122 - accuracy: 0.9820\n",
      "Epoch 140/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1118 - accuracy: 0.9820\n",
      "Epoch 141/150\n",
      "444/444 [==============================] - 0s 27us/step - loss: 0.1114 - accuracy: 0.9820\n",
      "Epoch 142/150\n",
      "444/444 [==============================] - 0s 45us/step - loss: 0.1110 - accuracy: 0.9820\n",
      "Epoch 143/150\n",
      "444/444 [==============================] - 0s 27us/step - loss: 0.1106 - accuracy: 0.9820\n",
      "Epoch 144/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1102 - accuracy: 0.9820\n",
      "Epoch 145/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1098 - accuracy: 0.9820\n",
      "Epoch 146/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1094 - accuracy: 0.9820\n",
      "Epoch 147/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1091 - accuracy: 0.9820\n",
      "Epoch 148/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1087 - accuracy: 0.9820\n",
      "Epoch 149/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1083 - accuracy: 0.9820\n",
      "Epoch 150/150\n",
      "444/444 [==============================] - 0s 36us/step - loss: 0.1079 - accuracy: 0.9820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2a14cc05f48>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, batch_size=100, nb_epoch=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the test results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred>0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[63  0]\n",
      " [ 6 42]]\n",
      "Testing Accuracy =  0.9459459459459459\n",
      "Recall =  0.9130434782608695\n",
      "Precision =  1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "TP = cm[0][0]  # Total Positive\n",
    "TN = cm[1][1]  # Total Negative\n",
    "FP = cm[0][1]  # False Positive\n",
    "FN = cm[1][0]  # False Negative\n",
    "print(cm)\n",
    "print('Testing Accuracy = ', (TP + TN)/ (TP + TN + FP + FN))\n",
    "print('Recall = ', TP/(TP + FN))\n",
    "print('Precision = ', TP/(TP + FP))\n",
    "print()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(cm, annot = True)\n",
    "plt.savefig('h.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
